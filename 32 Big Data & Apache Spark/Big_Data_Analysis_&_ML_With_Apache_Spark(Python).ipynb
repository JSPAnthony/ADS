{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TeD-FemPKiFV"
      },
      "source": [
        "Using the Spark distributed machine learning framework to build a scalable ML data pipeline\n",
        "\n",
        "\n",
        "\n",
        "*  Setting up Spark in the Google Colaboratory\n",
        "*   Machine Learning Basic Concepts\n",
        "*   Preprocessing and Data Transformation using Spark\n",
        "*   Spark Clustering with pyspark\n",
        "*   Classification with pyspark\n",
        "*   Regression methods with pyspark\n",
        "\n",
        "**What is Apache Spark?**\n",
        "\n",
        "\n",
        "\n",
        "*   Apache Spark is a unified computing engine and a set of libraries for parallel data processing on computer clusters\n",
        "*   Spark is the most actively developed open source engine for this task; making it the de facto tool for any developer or data scientist interested in big data.\n",
        "* Spark supports multiple widely used programming languages (Python, Java, Scala and R), includes libraries for diverse tasks ranging from SQL to streaming and machine learning\n",
        "* Runs anywhere from a laptop to a cluster of thousands of servers. This makes it an easy system to start with and scale up to big data processing or incredibly large scale.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SNOVifn2M3NV"
      },
      "source": [
        "Setting up Spark 3.0.1 in the Google Colaboratory\n",
        "\n",
        "We will install the below programs\n",
        "\n",
        "*   Java 8\n",
        "*   spark-3.0.1\n",
        "*   Hadoop3.2\n",
        "*   Findspark\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZcNbtGwJNb9T"
      },
      "source": [
        "you can install the LATEST version of Spark using the below set of commands."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "e_SpJDTfRjKW",
        "outputId": "4a2f9760-384f-4ded-babd-0e2f88264026",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 259 ms, sys: 64.8 ms, total: 324 ms\n",
            "Wall time: 33.9 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# install java\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "\n",
        "# install spark (change the version number if needed)\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.0.0/spark-3.0.0-bin-hadoop3.2.tgz\n",
        "\n",
        "# unzip the spark file to the current folder\n",
        "!tar xf spark-3.0.0-bin-hadoop3.2.tgz\n",
        "\n",
        "# set your spark folder to your system path environment. \n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.0-bin-hadoop3.2\"\n",
        "\n",
        "\n",
        "# install findspark using pip\n",
        "!pip install -q findspark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9ZG76YIPvDH"
      },
      "source": [
        "**Spark Installation test**\n",
        "Let us test the installation of spark in our google colab environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23c4O5ZWPxup",
        "outputId": "4455aa7a-2a8c-42b6-fc05-b4828e15f0d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/spark-3.0.0-bin-hadoop3.2/python/pyspark/sql/session.py:378: UserWarning: inferring schema from dict is deprecated,please use pyspark.sql.Row instead\n",
            "  warnings.warn(\"inferring schema from dict is deprecated,\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+\n",
            "|hello|\n",
            "+-----+\n",
            "|world|\n",
            "|world|\n",
            "|world|\n",
            "+-----+\n",
            "only showing top 3 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "# Test the spark \n",
        "df = spark.createDataFrame([{\"hello\": \"world\"} for x in range(1000)])\n",
        "\n",
        "df.show(3, False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m9jeEy6DQf-S",
        "outputId": "5a55fd40-dd96-4082-d6d5-464e0f9c2bcd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.0.0\n"
          ]
        }
      ],
      "source": [
        "# make sure the version of pyspark\n",
        "import pyspark\n",
        "print(pyspark.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NlSFOrywR8UD"
      },
      "source": [
        "Data Preparation and Transformations in Spark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8aCBX0OuSCfk"
      },
      "source": [
        "**Normalize Numeric Data**\n",
        "MinMaxScaler is one of the favorite classes shipped with most machine learning libraries. It scaled the data between 0 and 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pneIW3IASFkx",
        "outputId": "5f38739a-4f59-4b65-aa00-2b65b9c53930"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------------------+\n",
            "| id|          features|\n",
            "+---+------------------+\n",
            "|  1|[10.0,10000.0,1.0]|\n",
            "|  2|[20.0,30000.0,2.0]|\n",
            "|  3|[30.0,40000.0,3.0]|\n",
            "+---+------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.feature import MinMaxScaler\n",
        "from pyspark.ml.linalg import Vectors\n",
        "# Create some dummy feature data\n",
        "features_df = spark.createDataFrame([\n",
        "    (1, Vectors.dense([10.0,10000.0,1.0]),),\n",
        "    (2, Vectors.dense([20.0,30000.0,2.0]),),\n",
        "    (3, Vectors.dense([30.0,40000.0,3.0]),),\n",
        "    \n",
        "],[\"id\", \"features\"] )\n",
        "features_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nCCwhIHjSPwn",
        "outputId": "7e9172b7-5c69-468e-bb6c-4f4fe506f131"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------------------+--------------------+\n",
            "| id|          features|           sfeatures|\n",
            "+---+------------------+--------------------+\n",
            "|  1|[10.0,10000.0,1.0]|           (3,[],[])|\n",
            "|  2|[20.0,30000.0,2.0]|[0.5,0.6666666666...|\n",
            "|  3|[30.0,40000.0,3.0]|       [1.0,1.0,1.0]|\n",
            "+---+------------------+--------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Apply MinMaxScaler transformation\n",
        "features_scaler = MinMaxScaler(inputCol = \"features\", outputCol = \"sfeatures\")\n",
        "smodel = features_scaler.fit(features_df)\n",
        "sfeatures_df = smodel.transform(features_df)\n",
        "sfeatures_df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9WU9XVNSg-m"
      },
      "source": [
        "**Standardize Numeric Data**\n",
        "StandardScaler is another well-known class written with machine learning libraries. It normalizes the data between -1 and 1 and converts the data into bell-shaped data. You can demean the data and scale to some variance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4UA9d7txR-Kt",
        "outputId": "6c588cc3-e118-4bf8-8dd2-a6e776b18114"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------------------+\n",
            "| id|          features|\n",
            "+---+------------------+\n",
            "|  1|[10.0,10000.0,1.0]|\n",
            "|  2|[20.0,30000.0,2.0]|\n",
            "|  3|[30.0,40000.0,3.0]|\n",
            "+---+------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.feature import  StandardScaler\n",
        "from pyspark.ml.linalg import Vectors\n",
        "# Create the dummy data\n",
        "features_df = spark.createDataFrame([\n",
        "    (1, Vectors.dense([10.0,10000.0,1.0]),),\n",
        "    (2, Vectors.dense([20.0,30000.0,2.0]),),\n",
        "    (3, Vectors.dense([30.0,40000.0,3.0]),),\n",
        "    \n",
        "],[\"id\", \"features\"] )\n",
        "features_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cqchzwUJS6pV",
        "outputId": "d3dbdac9-7ff7-4dc9-825d-b7a7db1bd4ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------------------+--------------------+\n",
            "| id|          features|           sfeatures|\n",
            "+---+------------------+--------------------+\n",
            "|  1|[10.0,10000.0,1.0]|[-1.0,-1.09108945...|\n",
            "|  2|[20.0,30000.0,2.0]|[0.0,0.2182178902...|\n",
            "|  3|[30.0,40000.0,3.0]|[1.0,0.8728715609...|\n",
            "+---+------------------+--------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Apply the StandardScaler model\n",
        "features_stand_scaler = StandardScaler(inputCol = \"features\", outputCol = \"sfeatures\", withStd=True, withMean=True)\n",
        "stmodel = features_stand_scaler.fit(features_df)\n",
        "stand_sfeatures_df = stmodel.transform(features_df)\n",
        "stand_sfeatures_df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXeI74Z4TYQV"
      },
      "source": [
        "**Bucketize Numeric Data**\n",
        "\n",
        "The real data sets come with various ranges and sometimes it is advisable to transform the data into well-defined buckets before plugging into machine learning algorithms.\n",
        "Bucketizer class is handy to transform the data into various buckets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0GdljVOVTaz-",
        "outputId": "59f9aba4-d112-439c-e5d3-38c8041d429c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+\n",
            "|features|\n",
            "+--------+\n",
            "|  -800.0|\n",
            "|   -10.5|\n",
            "|    -1.7|\n",
            "|     0.0|\n",
            "|     8.2|\n",
            "|    90.1|\n",
            "+--------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.feature import  Bucketizer\n",
        "from pyspark.ml.linalg import Vectors\n",
        "# Define the splits for buckets\n",
        "splits = [-float(\"inf\"), -10, 0.0, 10, float(\"inf\")]\n",
        "b_data = [(-800.0,), (-10.5,), (-1.7,), (0.0,), (8.2,), (90.1,)]\n",
        "b_df = spark.createDataFrame(b_data, [\"features\"])\n",
        "b_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LzDCiPvLToBx",
        "outputId": "bf799685-b683-479b-bafc-8ae1075a9826"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+---------+\n",
            "|features|bfeatures|\n",
            "+--------+---------+\n",
            "|  -800.0|      0.0|\n",
            "|   -10.5|      0.0|\n",
            "|    -1.7|      1.0|\n",
            "|     0.0|      2.0|\n",
            "|     8.2|      2.0|\n",
            "|    90.1|      3.0|\n",
            "+--------+---------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Transforming data into buckets\n",
        "bucketizer = Bucketizer(splits=splits, inputCol= \"features\", outputCol=\"bfeatures\")\n",
        "bucketed_df = bucketizer.transform(b_df)\n",
        "bucketed_df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "blmMYeTLT6Jl"
      },
      "source": [
        "**Tokenize text Data**\n",
        "\n",
        "Natural Language Processing is one of the main applications of Machine learning. One of the first steps for NLP is tokenizing the text into words or token. We can utilize the Tokenizer class with SparkML to perform this task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K3Bc8eK_T8gg",
        "outputId": "eb11638b-a46c-4355-c40c-375f286bd517"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+--------------------+\n",
            "| id|           sentences|\n",
            "+---+--------------------+\n",
            "|  1|This is an introd...|\n",
            "|  2|Mlib incluse libr...|\n",
            "|  3|It also incluses ...|\n",
            "+---+--------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.feature import  Tokenizer\n",
        "sentences_df = spark.createDataFrame([\n",
        "    (1, \"This is an introduction to sparkMlib\"),\n",
        "    (2, \"Mlib incluse libraries fro classfication and regression\"),\n",
        "    (3, \"It also incluses support for data piplines\"),\n",
        "    \n",
        "], [\"id\", \"sentences\"])\n",
        "sentences_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rbmAdUdFUD5q",
        "outputId": "7912a1c3-a036-4854-e338-2cd7ec30abd2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(id=1, sentences='This is an introduction to sparkMlib', words=['this', 'is', 'an', 'introduction', 'to', 'sparkmlib']),\n",
              " Row(id=2, sentences='Mlib incluse libraries fro classfication and regression', words=['mlib', 'incluse', 'libraries', 'fro', 'classfication', 'and', 'regression']),\n",
              " Row(id=3, sentences='It also incluses support for data piplines', words=['it', 'also', 'incluses', 'support', 'for', 'data', 'piplines'])]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "sent_token = Tokenizer(inputCol = \"sentences\", outputCol = \"words\")\n",
        "sent_tokenized_df = sent_token.transform(sentences_df)\n",
        "sent_tokenized_df.take(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MF-NhnyJUgvN"
      },
      "source": [
        "**Clustering Using PySpark**\n",
        "\n",
        "Clustering is a machine learning technique where the data is grouped into a reasonable number of classes using the input features. In this section, we study the basic application of clustering techniques using the spark ML framework."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "SwCiEwLVUDXa"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.linalg import Vectors\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.clustering import KMeans, BisectingKMeans\n",
        "import glob\n",
        "# Downloading the clustering dataset\n",
        "!wget -q 'https://raw.githubusercontent.com/amjadraza/blogs-data/master/spark_ml/clustering_dataset.csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "TOEE8fhwUzDS",
        "outputId": "fef04773-ae26-4cfd-8ca0-c38f0774fbb3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+----+----+\n",
            "|col1|col2|col3|\n",
            "+----+----+----+\n",
            "|   7|   4|   1|\n",
            "|   7|   7|   9|\n",
            "|   7|   9|   6|\n",
            "|   1|   6|   5|\n",
            "|   6|   7|   7|\n",
            "|   7|   9|   4|\n",
            "|   7|  10|   6|\n",
            "|   7|   8|   2|\n",
            "|   8|   3|   8|\n",
            "|   4|  10|   5|\n",
            "|   7|   4|   5|\n",
            "|   7|   8|   4|\n",
            "|   2|   5|   1|\n",
            "|   2|   6|   2|\n",
            "|   2|   3|   8|\n",
            "|   3|   9|   1|\n",
            "|   4|   2|   9|\n",
            "|   1|   7|   1|\n",
            "|   6|   2|   3|\n",
            "|   4|   1|   9|\n",
            "+----+----+----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#  Load the clustering data stored in csv format using spark\n",
        "# Read the data.\n",
        "clustering_file_name ='clustering_dataset.csv'\n",
        "import pandas as pd\n",
        "# df = pd.read_csv(clustering_file_name)\n",
        "cluster_df = spark.read.csv(clustering_file_name, header=True,inferSchema=True)\n",
        "cluster_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EGu3xOGjVTqK",
        "outputId": "d0282ab0-f1f8-4b86-f556-d4f30fc52b00"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+----+----+--------------+\n",
            "|col1|col2|col3|      features|\n",
            "+----+----+----+--------------+\n",
            "|   7|   4|   1| [7.0,4.0,1.0]|\n",
            "|   7|   7|   9| [7.0,7.0,9.0]|\n",
            "|   7|   9|   6| [7.0,9.0,6.0]|\n",
            "|   1|   6|   5| [1.0,6.0,5.0]|\n",
            "|   6|   7|   7| [6.0,7.0,7.0]|\n",
            "|   7|   9|   4| [7.0,9.0,4.0]|\n",
            "|   7|  10|   6|[7.0,10.0,6.0]|\n",
            "|   7|   8|   2| [7.0,8.0,2.0]|\n",
            "|   8|   3|   8| [8.0,3.0,8.0]|\n",
            "|   4|  10|   5|[4.0,10.0,5.0]|\n",
            "+----+----+----+--------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Convert the tabular data into vectorized format using VectorAssembler\n",
        "\n",
        "# Coverting the input data into features column\n",
        "vectorAssembler = VectorAssembler(inputCols = ['col1', 'col2', 'col3'], outputCol = \"features\")\n",
        "vcluster_df = vectorAssembler.transform(cluster_df)\n",
        "vcluster_df.show(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6UMO1jcVgtl"
      },
      "source": [
        "Once the data is prepared into the format MLlib can use for models, now we can define and train the clustering algorithm such as K-Means. We can define the number of clusters and initialize the seed as done below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "TRSGipasViNb"
      },
      "outputs": [],
      "source": [
        "# Applying the k-means algorithm\n",
        "kmeans = KMeans().setK(3)\n",
        "kmeans = kmeans.setSeed(1)\n",
        "kmodel = kmeans.fit(vcluster_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "svqicvJ5VuFn",
        "outputId": "94fc3cb3-7947-481c-a6d8-e55c08c6ec23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The location of centers: [array([35.88461538, 31.46153846, 34.42307692]), array([80.        , 79.20833333, 78.29166667]), array([5.12, 5.84, 4.84])]\n"
          ]
        }
      ],
      "source": [
        "# After training has been finished, let us print the centers.\n",
        "centers = kmodel.clusterCenters()\n",
        "print(\"The location of centers: {}\".format(centers))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l0DcpxqYWN9H",
        "outputId": "29f6e5dc-b3fc-48e7-c325-fb04ad8a9b5e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([5.12, 5.84, 4.84]),\n",
              " array([35.88461538, 31.46153846, 34.42307692]),\n",
              " array([80.        , 79.20833333, 78.29166667])]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "# Applying Hierarchical Clustering\n",
        "bkmeans = BisectingKMeans().setK(3)\n",
        "bkmeans = bkmeans.setSeed(1)\n",
        "bkmodel = bkmeans.fit(vcluster_df)\n",
        "bkcneters = bkmodel.clusterCenters()\n",
        "bkcneters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6UW-xiXWgHG"
      },
      "source": [
        "Read More about Clustering : https://spark.apache.org/docs/3.0.1/ml-clustering.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2IHFARi6WxDG"
      },
      "source": [
        "**Classification Using PySpark**\n",
        "\n",
        "Classification is one of the widely used Machine algorithms and almost every data engineer and data scientist must know about these algorithms. Once the data is loaded and prepared, I will demonstrate three classification algorithms.\n",
        "\n",
        "NaiveBayes Classification\n",
        "Multi-Layer Perceptron Classification\n",
        "Decision Trees Classification\n",
        "\n",
        "We explore the supervised classification algorithms using IRIS data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "TLyMvYCWW9i_",
        "outputId": "f4fa49d5-5175-4a48-979b-7e9cc1968a16"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5.1</td>\n",
              "      <td>3.5</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4.9</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4.7</td>\n",
              "      <td>3.2</td>\n",
              "      <td>1.3</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4.6</td>\n",
              "      <td>3.1</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5.0</td>\n",
              "      <td>3.6</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     0    1    2    3            4\n",
              "0  5.1  3.5  1.4  0.2  Iris-setosa\n",
              "1  4.9  3.0  1.4  0.2  Iris-setosa\n",
              "2  4.7  3.2  1.3  0.2  Iris-setosa\n",
              "3  4.6  3.1  1.5  0.2  Iris-setosa\n",
              "4  5.0  3.6  1.4  0.2  Iris-setosa"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "# Downloading the clustering data\n",
        "!wget -q \"https://raw.githubusercontent.com/amjadraza/blogs-data/master/spark_ml/iris.csv\"\n",
        "df_iris = pd.read_csv(\"https://raw.githubusercontent.com/amjadraza/blogs-data/master/spark_ml/iris.csv\", header=None)\n",
        "df_iris.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l49luC4FXLnj",
        "outputId": "e67660d2-322f-4e0b-9529-321d6cabd41c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+---+---+---+-----------+\n",
            "|0  |1  |2  |3  |4          |\n",
            "+---+---+---+---+-----------+\n",
            "|5.1|3.5|1.4|0.2|Iris-setosa|\n",
            "|4.9|3.0|1.4|0.2|Iris-setosa|\n",
            "|4.7|3.2|1.3|0.2|Iris-setosa|\n",
            "|4.6|3.1|1.5|0.2|Iris-setosa|\n",
            "|5.0|3.6|1.4|0.2|Iris-setosa|\n",
            "+---+---+---+---+-----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import *\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.feature import  StringIndexer\n",
        "\n",
        "\n",
        "iris_df = spark.createDataFrame(df_iris)\n",
        "iris_df.show(5, False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "o9w6g24GX2FY"
      },
      "outputs": [],
      "source": [
        "# Rename the columns\n",
        "iris_df = iris_df.select(col(\"0\").alias(\"sepal_length\"),\n",
        "                         col(\"1\").alias(\"sepal_width\"),\n",
        "                         col(\"2\").alias(\"petal_length\"),\n",
        "                         col(\"3\").alias(\"petal_width\"),\n",
        "                         col(\"4\").alias(\"species\"),\n",
        "                        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tBoQKetfX4a-",
        "outputId": "7fc0daf2-ff70-4a69-c09a-906c7c738f79"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+-----------+------------+-----------+-----------+-----------------+\n",
            "|sepal_length|sepal_width|petal_length|petal_width|species    |features         |\n",
            "+------------+-----------+------------+-----------+-----------+-----------------+\n",
            "|5.1         |3.5        |1.4         |0.2        |Iris-setosa|[5.1,3.5,1.4,0.2]|\n",
            "|4.9         |3.0        |1.4         |0.2        |Iris-setosa|[4.9,3.0,1.4,0.2]|\n",
            "|4.7         |3.2        |1.3         |0.2        |Iris-setosa|[4.7,3.2,1.3,0.2]|\n",
            "|4.6         |3.1        |1.5         |0.2        |Iris-setosa|[4.6,3.1,1.5,0.2]|\n",
            "|5.0         |3.6        |1.4         |0.2        |Iris-setosa|[5.0,3.6,1.4,0.2]|\n",
            "+------------+-----------+------------+-----------+-----------+-----------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Converting the columns into features\n",
        "vectorAssembler = VectorAssembler(inputCols = [\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\"],\n",
        "                                  outputCol = \"features\")\n",
        "viris_df = vectorAssembler.transform(iris_df)\n",
        "viris_df.show(5, False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eDuOHeJwYIB0",
        "outputId": "a7384f12-ec67-4007-bdd5-cffe10604b53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+-----------+------------+-----------+-----------+-----------------+-----+\n",
            "|sepal_length|sepal_width|petal_length|petal_width|species    |features         |label|\n",
            "+------------+-----------+------------+-----------+-----------+-----------------+-----+\n",
            "|5.1         |3.5        |1.4         |0.2        |Iris-setosa|[5.1,3.5,1.4,0.2]|0.0  |\n",
            "|4.9         |3.0        |1.4         |0.2        |Iris-setosa|[4.9,3.0,1.4,0.2]|0.0  |\n",
            "|4.7         |3.2        |1.3         |0.2        |Iris-setosa|[4.7,3.2,1.3,0.2]|0.0  |\n",
            "|4.6         |3.1        |1.5         |0.2        |Iris-setosa|[4.6,3.1,1.5,0.2]|0.0  |\n",
            "|5.0         |3.6        |1.4         |0.2        |Iris-setosa|[5.0,3.6,1.4,0.2]|0.0  |\n",
            "+------------+-----------+------------+-----------+-----------+-----------------+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "indexer = StringIndexer(inputCol=\"species\", outputCol = \"label\")\n",
        "iviris_df = indexer.fit(viris_df).transform(viris_df)\n",
        "iviris_df.show(5, False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wnq3H7pnYbfo"
      },
      "source": [
        "**Naive Bayes Classification**\n",
        "\n",
        "Once the data is prepared, we are ready to apply the first classification algorithm."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ch5g1-m3Yd8e",
        "outputId": "c9bea882-b8d3-46d5-a75a-ba20a2b28f7b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[DataFrame[sepal_length: double, sepal_width: double, petal_length: double, petal_width: double, species: string, features: vector, label: double],\n",
              " DataFrame[sepal_length: double, sepal_width: double, petal_length: double, petal_width: double, species: string, features: vector, label: double]]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "from pyspark.ml.classification import NaiveBayes\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "# Create the traing and test splits\n",
        "splits = iviris_df.randomSplit([0.7,0.3], 1)\n",
        "splits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t-f_1hiLY4le",
        "outputId": "abe1356a-8585-4f0d-fbf8-cfa68ec6f590"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[sepal_length: double, sepal_width: double, petal_length: double, petal_width: double, species: string, features: vector, label: double]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "train_df = splits[0]\n",
        "train_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "szOVilpXY_x_"
      },
      "outputs": [],
      "source": [
        "test_df = splits[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dA1qyC-bZGPX",
        "outputId": "b2049c27-532e-4bc9-f74b-44657c001db1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+-----------+------------+-----------+-----------+-----------------+-----+-------------------------------------------------------------+----------------------------------------------------------+----------+\n",
            "|sepal_length|sepal_width|petal_length|petal_width|species    |features         |label|rawPrediction                                                |probability                                               |prediction|\n",
            "+------------+-----------+------------+-----------+-----------+-----------------+-----+-------------------------------------------------------------+----------------------------------------------------------+----------+\n",
            "|4.5         |2.3        |1.3         |0.3        |Iris-setosa|[4.5,2.3,1.3,0.3]|0.0  |[-10.425727021464883,-11.066875873762312,-11.568932156608787]|[0.5418636866103503,0.2853925545102739,0.1727437588793758]|0.0       |\n",
            "+------------+-----------+------------+-----------+-----------+-----------------+-----+-------------------------------------------------------------+----------------------------------------------------------+----------+\n",
            "only showing top 1 row\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Apply the Naive bayes classifier\n",
        "nb = NaiveBayes(modelType=\"multinomial\")\n",
        "nbmodel = nb.fit(train_df)\n",
        "predictions_df = nbmodel.transform(test_df)\n",
        "predictions_df.show(1, False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H2jGdojGZaE5",
        "outputId": "b8a430ec-7bd1-4875-9b53-753cb0b16001"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8275862068965517"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "# Let us Evaluate the trained classifier\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "nbaccuracy = evaluator.evaluate(predictions_df)\n",
        "nbaccuracy\n",
        "0.8275862068965517"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FKqI0VX_ZnaI"
      },
      "source": [
        "**Multilayer Perceptron Classification**\n",
        "\n",
        "The second classifier we will be investigating is a Multi-layer perceptron. In this tutorial, I am not going into details of the optimal MLP network for this problem however in practice, you research the optimal network suitable to the problem in hand."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A1O2TfeWZqUr",
        "outputId": "6cb1ba75-18ac-47d9-8368-1cba18219615"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9761904761904762"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
        "# Define the MLP Classifier\n",
        "layers = [4,5,5,3]\n",
        "mlp = MultilayerPerceptronClassifier(layers = layers, seed=1)\n",
        "mlp_model = mlp.fit(train_df)\n",
        "mlp_predictions = mlp_model.transform(test_df)\n",
        "# Evaluate the MLP classifier\n",
        "mlp_evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "mlp_accuracy = mlp_evaluator.evaluate(mlp_predictions)\n",
        "mlp_accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQ2s8IYLZ8O1"
      },
      "source": [
        "**Decision Trees Classification**\n",
        "\n",
        "Another common classifier in the ML family is the Decision Tree Classifier, in this section, we explore this classifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HOwLgqAcZ_Wp",
        "outputId": "dc6a171e-82fb-4e1c-925e-8acb4873d28c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9761904761904762"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "from pyspark.ml.classification import DecisionTreeClassifier\n",
        "# Define the DT Classifier \n",
        "dt = DecisionTreeClassifier(labelCol=\"label\", featuresCol=\"features\")\n",
        "dt_model = dt.fit(train_df)\n",
        "dt_predictions = dt_model.transform(test_df)\n",
        "# Evaluate the DT Classifier\n",
        "dt_evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "dt_accuracy = dt_evaluator.evaluate(dt_predictions)\n",
        "dt_accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Le0GMNFQaesr"
      },
      "source": [
        "**Regression using PySpark**\n",
        "\n",
        "\n",
        "In this section, we explore the Machine learning models for regression problems using pyspark. Regression models are helpful in predicting future values using past data.\n",
        "We will use the Combined Cycle Power Plant data set to predict the net hourly electrical output (EP). I have uploaded the data to my GitHub so that users can reproduce the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7X7w8NdFajxy",
        "outputId": "d3d205b2-b741-47a2-d13d-edecd1e368b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----+-------+-----+------+\n",
            "|AT   |V    |AP     |RH   |PE    |\n",
            "+-----+-----+-------+-----+------+\n",
            "|14.96|41.76|1024.07|73.17|463.26|\n",
            "|25.18|62.96|1020.04|59.08|444.37|\n",
            "+-----+-----+-------+-----+------+\n",
            "only showing top 2 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.regression import LinearRegression\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "# Read the iris data\n",
        "df_ccpp = pd.read_csv(\"https://raw.githubusercontent.com/amjadraza/blogs-data/master/spark_ml/ccpp.csv\")\n",
        "pp_df = spark.createDataFrame(df_ccpp)\n",
        "pp_df.show(2, False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JJ9CNb-nayFt",
        "outputId": "869a17db-db57-42c4-9ef3-249edf02a54c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----+-------+-----+------+---------------------------+\n",
            "|AT   |V    |AP     |RH   |PE    |features                   |\n",
            "+-----+-----+-------+-----+------+---------------------------+\n",
            "|14.96|41.76|1024.07|73.17|463.26|[14.96,41.76,1024.07,73.17]|\n",
            "|25.18|62.96|1020.04|59.08|444.37|[25.18,62.96,1020.04,59.08]|\n",
            "+-----+-----+-------+-----+------+---------------------------+\n",
            "only showing top 2 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Create the feature column using VectorAssembler class\n",
        "vectorAssembler = VectorAssembler(inputCols =[\"AT\", \"V\", \"AP\", \"RH\"], outputCol = \"features\")\n",
        "vpp_df = vectorAssembler.transform(pp_df)\n",
        "vpp_df.show(2, False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aGy7l9eda6aF"
      },
      "source": [
        "**Linear Regression**\n",
        "\n",
        "We start with the simplest regression technique i.e. Linear Regression."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FoZ4NDk-a-UB",
        "outputId": "00ee5344-a327-4eef-d5c4-35e2ce2d5e10"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.557126016749486"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ],
      "source": [
        "# Define and fit Linear Regression\n",
        "lr = LinearRegression(featuresCol=\"features\", labelCol=\"PE\")\n",
        "lr_model = lr.fit(vpp_df)\n",
        "# Print and save the Model output\n",
        "lr_model.coefficients\n",
        "lr_model.intercept\n",
        "lr_model.summary.rootMeanSquaredError"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ceQlGXDrbIZM"
      },
      "source": [
        "**Decision Tree Regression**\n",
        "\n",
        "In this section, we explore the Decision Tree Regression commonly used in Machine learning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EONvPQeLbY_y",
        "outputId": "3863bda6-4ffb-41e9-e0f7-2c203a9e2c9f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----+-------+-----+------+---------------------------+\n",
            "|AT   |V    |AP     |RH   |PE    |features                   |\n",
            "+-----+-----+-------+-----+------+---------------------------+\n",
            "|14.96|41.76|1024.07|73.17|463.26|[14.96,41.76,1024.07,73.17]|\n",
            "|25.18|62.96|1020.04|59.08|444.37|[25.18,62.96,1020.04,59.08]|\n",
            "+-----+-----+-------+-----+------+---------------------------+\n",
            "only showing top 2 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.regression import DecisionTreeRegressor\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "vpp_df.show(2, False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "fKqg10tlbNcT"
      },
      "outputs": [],
      "source": [
        "# Define train and test data split\n",
        "splits = vpp_df.randomSplit([0.7,0.3])\n",
        "train_df = splits[0]\n",
        "test_df = splits[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TgU7XEqubf7Z",
        "outputId": "67509ac9-5733-4190-ac4c-6822e9ff97cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----+-------+-----+------+-------------------------+------------------+\n",
            "|AT |V    |AP     |RH   |PE    |features                 |prediction        |\n",
            "+---+-----+-------+-----+------+-------------------------+------------------+\n",
            "|2.8|39.64|1011.01|82.96|482.66|[2.8,39.64,1011.01,82.96]|485.49412140575083|\n",
            "+---+-----+-------+-----+------+-------------------------+------------------+\n",
            "only showing top 1 row\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Define the Decision Tree Model \n",
        "dt = DecisionTreeRegressor(featuresCol=\"features\", labelCol=\"PE\")\n",
        "dt_model = dt.fit(train_df)\n",
        "dt_predictions = dt_model.transform(test_df)\n",
        "dt_predictions.show(1, False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RcFfYjqubnvz",
        "outputId": "872d16be-6a57-4639-b526-1cb23786bf0e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The RMSE of Decision Tree regression Model is 4.511558972196385\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the Model\n",
        "dt_evaluator = RegressionEvaluator(labelCol=\"PE\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
        "dt_rmse = dt_evaluator.evaluate(dt_predictions)\n",
        "print(\"The RMSE of Decision Tree regression Model is {}\".format(dt_rmse))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-A03ALSpc2qN"
      },
      "source": [
        "**Gradient Boosting Decision Tree Regression**\n",
        "\n",
        "Gradient Boosting is another common choice among ML professionals. Let us try the GBM in this section."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pqySn2O8c423",
        "outputId": "739c2ae5-486f-41b1-d84e-e4ae64d54ca8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The RMSE of GBT Tree regression Model is 4.091406348123477\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.regression import GBTRegressor\n",
        "# Define the GBT Model\n",
        "gbt = GBTRegressor(featuresCol=\"features\", labelCol=\"PE\")\n",
        "gbt_model = gbt.fit(train_df)\n",
        "gbt_predictions = gbt_model.transform(test_df)\n",
        "# Evaluate the GBT Model\n",
        "gbt_evaluator = RegressionEvaluator(labelCol=\"PE\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
        "gbt_rmse = gbt_evaluator.evaluate(gbt_predictions)\n",
        "print(\"The RMSE of GBT Tree regression Model is {}\".format(gbt_rmse))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7xmXp_ONdE9c"
      },
      "source": [
        "References Readings/Links\n",
        "\n",
        "https://spark.apache.org/docs/latest/ml-features.html\n",
        "\n",
        "https://spark.apache.org/docs/3.0.1/ml-classification-regression.html#regression\n",
        "\n",
        "https://spark.apache.org/docs/3.0.1/ml-clustering.html\n",
        "\n",
        "https://spark.apache.org/docs/3.0.1/ml-classification-regression.html#classification"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Big Data Analysis & ML With  Apache Spark(Python).ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}